#!/usr/bin/env python3
"""
å®éªŒè¿è¡Œè„šæœ¬ï¼šæ‰¹é‡è¿è¡Œä¸åŒé…ç½®çš„å®éªŒ
"""
import subprocess
import sys
import time


def run_command(cmd):
    """è¿è¡Œå‘½ä»¤å¹¶æ‰“å°ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
    print('='*60)
    
    start_time = time.time()
    result = subprocess.run(cmd, capture_output=False, text=True)
    end_time = time.time()
    
    print(f"\nå‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.1f}ç§’")
    print(f"è¿”å›ç : {result.returncode}")
    
    return result.returncode == 0


def main():
    """è¿è¡Œä¸€ç³»åˆ—å®éªŒ"""
    
    experiments = [
        
        # ============================================================================
        # TokenPool çº¯æ³¨æ„åŠ›æ± åŒ–å®éªŒ - æ— CNNå‰ç«¯ï¼Œç›´æ¥å­¦ä¹ æ—¶é—´ç‰¹å¾
        # ============================================================================
        
        # {
        #     "name": "ğŸŸ© FD002-A: æé«˜tokenç²’åº¦ï¼ˆpatch=4ï¼‰",
        #     "cmd": ["python", "train.py",
        #            "--model", "tokenpool",
        #            "--fault", "FD002",
        #            "--batch_size", "192",
        #            "--epochs", "100",
        #            "--learning_rate", "0.0008",
        #            "--weight_decay", "0.0002",
        #            "--patch", "4",
        #            "--d_model", "160",
        #            "--depth", "5",
        #            "--token_mlp_dim", "320",
        #            "--channel_mlp_dim", "160",
        #            "--dropout", "0.12",
        #            "--cnn_pool", "weighted",
        #            "--tokenpool_heads", "8",
        #            "--tokenpool_dropout", "0.12",
        #            "--tokenpool_temperature", "1.8",
        #            "--scheduler", "cosine",
        #            "--early_stopping", "12"
        #            ]
        # },
        
        {
            "name": "ğŸŸ¦ FD002-B: å¤šå¤´åˆ†å·¥ï¼ˆheads=10, æ¸©åº¦2.0ï¼‰",
            "cmd": ["python", "train.py",
                   "--model", "tokenpool",
                   "--fault", "FD002",
                   "--batch_size", "192",
                   "--epochs", "100",
                   "--learning_rate", "0.0008",
                   "--weight_decay", "0.0002",
                   "--patch", "5",
                   "--d_model", "200",
                   "--depth", "5",
                   "--token_mlp_dim", "400",
                   "--channel_mlp_dim", "200",
                   "--dropout", "0.12",
                   "--cnn_pool", "weighted",
                   "--tokenpool_heads", "10",
                   "--tokenpool_dropout", "0.12",
                   "--tokenpool_temperature", "2.0",
                   "--scheduler", "cosine",
                   "--early_stopping", "12"
                   ]
        },
        
        {
            "name": "ğŸŸ¨ FD002-C: æ›´ç»†ç²’åº¦ä¸Šé™ï¼ˆpatch=3ï¼‰",
            "cmd": ["python", "train.py",
                   "--model", "tokenpool",
                   "--fault", "FD002",
                   "--batch_size", "160",
                   "--epochs", "100",
                   "--learning_rate", "0.0008",
                   "--weight_decay", "0.0002",
                   "--patch", "3",
                   "--d_model", "160",
                   "--depth", "5",
                   "--token_mlp_dim", "384",
                   "--channel_mlp_dim", "160",
                   "--dropout", "0.14",
                   "--cnn_pool", "weighted",
                   "--tokenpool_heads", "8",
                   "--tokenpool_dropout", "0.14",
                   "--tokenpool_temperature", "1.9",
                   "--scheduler", "cosine",
                   "--early_stopping", "12"
                   ]
        },
        
        {
            "name": "ğŸŸª FD002-D: å¤§å®¹é‡ç¨³æ€ï¼ˆd_modelâ†‘, depthâ†‘ï¼‰",
            "cmd": ["python", "train.py",
                   "--model", "tokenpool",
                   "--fault", "FD002",
                   "--batch_size", "160",
                   "--epochs", "100",
                   "--learning_rate", "0.0007",
                   "--weight_decay", "0.00025",
                   "--patch", "5",
                   "--d_model", "192",
                   "--depth", "6",
                   "--token_mlp_dim", "384",
                   "--channel_mlp_dim", "192",
                   "--dropout", "0.13",
                   "--cnn_pool", "weighted",
                   "--tokenpool_heads", "8",
                   "--tokenpool_dropout", "0.12",
                   "--tokenpool_temperature", "1.7",
                   "--scheduler", "cosine",
                   "--early_stopping", "14"
                   ]
        },
        
        {
            "name": "ğŸŸ¥ FD002-E: æœ«ç«¯æ›´å…³æ³¨ï¼ˆpool=last å¯¹ç…§ï¼‰",
            "cmd": ["python", "train.py",
                   "--model", "tokenpool",
                   "--fault", "FD002",
                   "--batch_size", "192",
                   "--epochs", "100",
                   "--learning_rate", "0.0008",
                   "--weight_decay", "0.0002",
                   "--patch", "4",
                   "--d_model", "160",
                   "--depth", "5",
                   "--token_mlp_dim", "320",
                   "--channel_mlp_dim", "160",
                   "--dropout", "0.12",
                   "--cnn_pool", "last",
                   "--tokenpool_heads", "8",
                   "--tokenpool_dropout", "0.12",
                   "--tokenpool_temperature", "1.8",
                   "--scheduler", "plateau",
                   "--early_stopping", "12"
                   ]
        },
    
        
        {
            "name": "âš¡ TokenPool-3: FD004æé™æŒ‘æˆ˜",
            "cmd": ["python", "train.py", 
                   "--model", "tokenpool", 
                   "--fault", "FD004", 
                   "--batch_size", "128",           # FD004æœ€å¤æ‚ï¼Œå°æ‰¹é‡
                   "--epochs", "100",                # FD004éœ€è¦å……åˆ†è®­ç»ƒ
                   "--learning_rate", "0.0006",     # FD004ä¿å®ˆå­¦ä¹ ç‡
                   "--weight_decay", "0.0003",      # å¼ºæƒé‡è¡°å‡
                   # TokenPoolå‚æ•° - ä¸ºå¤æ‚æ•°æ®é›†ä¼˜åŒ–
                   "--patch", "5",                  # ä¿æŒ10ä¸ªtokens
                   "--d_model", "160",              # æ›´å¤§æ¨¡å‹å®¹é‡
                   "--depth", "6",                  # æ·±å±‚TSMixer
                   "--token_mlp_dim", "384",        # å¤§MLP
                   "--channel_mlp_dim", "192",
                   "--dropout", "0.15",             # å¼ºdropouté˜²è¿‡æ‹Ÿåˆ
                   "--cnn_pool", "weighted",        # å…³æ³¨åæœŸç‰¹å¾
                   "--tokenpool_heads", "8",        # æ›´å¤šæ³¨æ„åŠ›å¤´å¤„ç†å¤æ‚æ¨¡å¼
                   "--tokenpool_dropout", "0.15",   
                   "--tokenpool_temperature", "2.0", # æ›´é«˜æ¸©åº¦é˜²å¡Œç¼©
                   "--scheduler", "cosine",
                   "--early_stopping", "15"
                   ]
        },
        
        # ============================================================================
        # CNN-TSMixer vs TSMixer å¯¹æ¯”å®éªŒ - ç›®æ ‡è¶…è¶Š 11.39 RMSE (TSMixeræœ€ä½³)
        # ============================================================================
        
        # {
        #     "name": "ğŸ¯ CNN-TSMixer-1: å¯¹æ ‡å† å†›é…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "384",           # å¯¹æ ‡TSMixerå† å†›
        #            "--epochs", "50",
        #            "--learning_rate", "0.005",      # å¯¹æ ‡é«˜å­¦ä¹ ç‡
        #            "--weight_decay", "0.00005",     # å¯¹æ ‡ä½æƒé‡è¡°å‡
        #            # CNN-TSMixerç‰¹å®šå‚æ•°
        #            "--patch", "5",                  # é€‚ä¸­patch size
        #            "--cnn_channels", "64",
        #            "--cnn_layers", "2",             # è½»é‡CNNå‰ç«¯
        #            "--cnn_kernel", "5",
        #            "--d_model", "128",
        #            "--depth", "4",                  # å¯¹æ ‡TSMixerå±‚æ•°
        #            "--token_mlp_dim", "256",
        #            "--channel_mlp_dim", "128",
        #            "--dropout", "0.05",             # å¯¹æ ‡æä½dropout
        #            "--cnn_pool", "mean",
        #            "--scheduler", "plateau",
        #            "--early_stopping", "10"
        #            ]
        # },
        
        # {
        #     "name": "ğŸš€ CNN-TSMixer-2: å¢å¼ºCNNå‰ç«¯",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",           # é€‚ä¸­æ‰¹é‡é€‚åº”æ›´å¤æ‚æ¨¡å‹
        #            "--epochs", "60",
        #            "--learning_rate", "0.003",      # ç¨ä½å­¦ä¹ ç‡é€‚åº”å¤æ‚åº¦
        #            "--weight_decay", "0.0001",
        #            # å¼ºåŒ–CNNç‰¹å¾æå–
        #            "--patch", "4",                  # æ›´ç»†ç²’åº¦patch
        #            "--cnn_channels", "80",          # æ›´å¤šCNNé€šé“
        #            "--cnn_layers", "3",             # æ›´æ·±CNN
        #            "--cnn_kernel", "7",             # æ›´å¤§å·ç§¯æ ¸
        #            "--d_model", "160",              # æ›´å¤§æ¨¡å‹ç»´åº¦
        #            "--depth", "4",
        #            "--token_mlp_dim", "320",
        #            "--channel_mlp_dim", "160",
        #            "--dropout", "0.08",             # é€‚å½“å¢åŠ æ­£åˆ™åŒ–
        #            "--cnn_pool", "weighted",        # å…³æ³¨åæœŸç‰¹å¾
        #            "--scheduler", "plateau",
        #            "--early_stopping", "12"
        #            ]
        # },
        
        # {
        #     "name": "âš¡ CNN-TSMixer-3: é«˜æ•ˆè½»é‡ç‰ˆ",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "512",           # å¤§æ‰¹é‡å¿«é€Ÿè®­ç»ƒ
        #            "--epochs", "40",
        #            "--learning_rate", "0.006",      # æ›´é«˜å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›
        #            "--weight_decay", "0.00003",     # æ›´ä½æƒé‡è¡°å‡
        #            # è½»é‡é«˜æ•ˆé…ç½®
        #            "--patch", "8",                  # å¤§patchå‡å°‘è®¡ç®—
        #            "--cnn_channels", "48",          # è½»é‡CNN
        #            "--cnn_layers", "2",
        #            "--cnn_kernel", "3",             # å°å·ç§¯æ ¸
        #            "--d_model", "96",               # å°æ¨¡å‹ç»´åº¦
        #            "--depth", "3",                  # æµ…å±‚TSMixer
        #            "--token_mlp_dim", "192",
        #            "--channel_mlp_dim", "96",
        #            "--dropout", "0.03",             # æä½dropout
        #            "--cnn_pool", "last",            # å…³æ³¨æœ€ç»ˆçŠ¶æ€
        #            "--scheduler", "plateau",
        #            "--early_stopping", "8"
        #            ]
        # },
        
        # {
        #     "name": "ğŸ”¥ CNN-TSMixer-4: æ·±åº¦æ··åˆæ¶æ„",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "192",           # ä¸­ç­‰æ‰¹é‡é€‚åº”æ·±å±‚ç½‘ç»œ
        #            "--epochs", "80",
        #            "--learning_rate", "0.002",      # æ·±å±‚ç½‘ç»œä¿å®ˆå­¦ä¹ ç‡
        #            "--weight_decay", "0.0002",
        #            # æ·±å±‚æ··åˆæ¶æ„
        #            "--patch", "5",
        #            "--cnn_channels", "96",          # ä¸°å¯ŒCNNç‰¹å¾
        #            "--cnn_layers", "4",             # æ·±å±‚CNN
        #            "--cnn_kernel", "5",
        #            "--d_model", "192",              # å¤§æ¨¡å‹å®¹é‡
        #            "--depth", "6",                  # æ·±å±‚TSMixer
        #            "--token_mlp_dim", "384",
        #            "--channel_mlp_dim", "192",
        #            "--dropout", "0.10",             # é€‚ä¸­æ­£åˆ™åŒ–
        #            "--cnn_pool", "weighted",
        #            "--scheduler", "onecycle",       # æ·±å±‚ç½‘ç»œç”¨onecycle
        #            "--early_stopping", "15"
        #            ]
        # },
        
        # {
        #     "name": "ğŸ¨ CNN-TSMixer-5: ç²¾ç»†è°ƒä¼˜ç‰ˆ",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "320",           # ç²¾å¿ƒé€‰æ‹©çš„æ‰¹é‡å¤§å°
        #            "--epochs", "70",
        #            "--learning_rate", "0.0035",     # ç²¾è°ƒå­¦ä¹ ç‡
        #            "--weight_decay", "0.00008",     # ç²¾è°ƒæƒé‡è¡°å‡
        #            # ç²¾ç»†è°ƒä¼˜å‚æ•°
        #            "--patch", "6",                  # å¹³è¡¡çš„patch size
        #            "--cnn_channels", "72",          # ç²¾è°ƒé€šé“æ•°
        #            "--cnn_layers", "3",
        #            "--cnn_kernel", "6",             # å¶æ•°å·ç§¯æ ¸
        #            "--d_model", "144",              # 144 = 72 * 2
        #            "--depth", "5",                  # ä¸­ç­‰æ·±åº¦
        #            "--token_mlp_dim", "288",        # 144 * 2
        #            "--channel_mlp_dim", "144",
        #            "--dropout", "0.06",             # ç²¾è°ƒdropout
        #            "--cnn_pool", "mean",
        #            "--scheduler", "plateau",
        #            "--early_stopping", "12"
        #            ]
        # },
        
        # {
        #     "name": "ğŸ† CNN-TSMixer-6: æé™æŒ‘æˆ˜ç‰ˆ",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "128",           # å°æ‰¹é‡æ”¯æŒå¤§æ¨¡å‹
        #            "--epochs", "100",
        #            "--learning_rate", "0.001",      # ä¿å®ˆå­¦ä¹ ç‡é•¿æœŸè®­ç»ƒ
        #            "--weight_decay", "0.0003",
        #            # æé™é…ç½®
        #            "--patch", "3",                  # æœ€ç»†ç²’åº¦
        #            "--cnn_channels", "128",         # æœ€å¤šCNNé€šé“
        #            "--cnn_layers", "4",             # æ·±å±‚CNN
        #            "--cnn_kernel", "7",             # å¤§å·ç§¯æ ¸
        #            "--d_model", "256",              # æœ€å¤§æ¨¡å‹ç»´åº¦
        #            "--depth", "8",                  # æœ€æ·±TSMixer
        #            "--token_mlp_dim", "512",        # æœ€å¤§MLP
        #            "--channel_mlp_dim", "256",
        #            "--dropout", "0.12",             # å¼ºæ­£åˆ™åŒ–é˜²è¿‡æ‹Ÿåˆ
        #            "--cnn_pool", "weighted",
        #            "--scheduler", "cosine",         # é•¿æœŸè®­ç»ƒç”¨cosine
        #            "--early_stopping", "20"
        #            ]
        # },

        # ============================================================================
        # é—¨æ§CNN-TSMixerå®éªŒ - è‡ªé€‚åº”ç‰¹å¾èåˆæ¶æ„
        # ============================================================================
        
        # {
        #     "name": "ğŸ”¥ é—¨æ§CNN-TSMixer-1: å¯¹æ ‡TSMixerå† å†›",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD001", 
        #            "--batch_size", "384",           # å¯¹æ ‡TSMixerå† å†›é…ç½®
        #            "--epochs", "50",
        #            "--learning_rate", "0.005",      # é«˜å­¦ä¹ ç‡
        #            "--weight_decay", "0.00005",     # æä½æƒé‡è¡°å‡
        #            # é—¨æ§CNN-TSMixerå‚æ•°
        #            "--patch", "5",                  # é€‚ä¸­patch
        #            "--cnn_channels", "64",          # æ ‡å‡†CNNé€šé“
        #            "--cnn_layers", "2",             # è½»é‡CNNå‰ç«¯
        #            "--cnn_kernel", "3",             # å°å·ç§¯æ ¸ä¿æŒé•¿åº¦
        #            "--d_model", "128",
        #            "--depth", "4",                  # å¯¹æ ‡TSMixerå±‚æ•°
        #            "--token_mlp_dim", "256",
        #            "--channel_mlp_dim", "128",
        #            "--dropout", "0.05",             # æä½dropout
        #            "--cnn_pool", "mean",
        #            "--use_groupnorm",               # ä½¿ç”¨GroupNorm
        #            "--gn_groups", "8",              # 8ç»„GroupNorm
        #            "--scheduler", "plateau",
        #            "--early_stopping", "10"
        #            ]
        # },
        
        

        # ============================================================================
        # é—¨æ§CNN-TSMixeræœ€ä½³é…ç½® - å…¨æ•°æ®é›†æµ‹è¯• (åŸºäºFD001æœ€ä¼˜ç»“æœ11.23 RMSE)
        # ============================================================================
        
        # {
        #     "name": "ğŸ¥‡ é—¨æ§CNN-TSMixeræœ€ä½³é…ç½® + FD001",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD001", 
        #            "--batch_size", "384",           # æœ€ä½³é…ç½®
        #            "--epochs", "50",
        #            "--learning_rate", "0.005",      # é«˜å­¦ä¹ ç‡
        #            "--weight_decay", "0.00005",     # æä½æƒé‡è¡°å‡
        #            # é—¨æ§CNN-TSMixeræœ€ä½³å‚æ•°
        #            "--patch", "5",                  # æœ€ä½³patch size
        #            "--cnn_channels", "64",          # æœ€ä½³CNNé€šé“æ•°
        #            "--cnn_layers", "2",             # æœ€ä½³CNNå±‚æ•°
        #            "--cnn_kernel", "3",             # æœ€ä½³å·ç§¯æ ¸å¤§å°
        #            "--d_model", "128",              # æœ€ä½³æ¨¡å‹ç»´åº¦
        #            "--depth", "4",                  # æœ€ä½³TSMixerå±‚æ•°
        #            "--token_mlp_dim", "256",        # æœ€ä½³Token MLPç»´åº¦
        #            "--channel_mlp_dim", "128",      # æœ€ä½³Channel MLPç»´åº¦
        #            "--dropout", "0.05",             # æœ€ä½³dropout
        #            "--cnn_pool", "mean",            # æœ€ä½³æ± åŒ–æ–¹å¼
        #            "--use_groupnorm",               # ä½¿ç”¨GroupNorm
        #            "--gn_groups", "8",              # æœ€ä½³GroupNormåˆ†ç»„
        #            "--scheduler", "plateau",        # æœ€ä½³è°ƒåº¦å™¨
        #            "--early_stopping", "10"         # æœ€ä½³æ—©åœç­–ç•¥
        #            ]
        # },
        
        # {
        #     "name": "ğŸ¥‡ é—¨æ§CNN-TSMixeræœ€ä½³é…ç½® + FD002",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD002", 
        #            "--batch_size", "256",           # FD002å¤æ‚æ•°æ®é€‚å½“å‡å°æ‰¹é‡
        #            "--epochs", "60",                # FD002éœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°
        #            "--learning_rate", "0.003",      # FD002ç”¨ç¨ä½å­¦ä¹ ç‡
        #            "--weight_decay", "0.0001",      # FD002å¢åŠ æƒé‡è¡°å‡
        #            # é—¨æ§CNN-TSMixeræœ€ä½³å‚æ•°
        #            "--patch", "5",                  
        #            "--cnn_channels", "64",          
        #            "--cnn_layers", "2",             
        #            "--cnn_kernel", "3",             
        #            "--d_model", "128",              
        #            "--depth", "4",                  
        #            "--token_mlp_dim", "256",        
        #            "--channel_mlp_dim", "128",      
        #            "--dropout", "0.08",             # FD002å¢åŠ dropouté˜²è¿‡æ‹Ÿåˆ
        #            "--cnn_pool", "mean",            
        #            "--use_groupnorm",               
        #            "--gn_groups", "8",              
        #            "--scheduler", "plateau",        
        #            "--early_stopping", "12"         # FD002å¢åŠ æ—©åœè€å¿ƒå€¼
        #            ]
        # },
        
        # {
        #     "name": "ğŸ¥‡ é—¨æ§CNN-TSMixeræœ€ä½³é…ç½® + FD003",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD003", 
        #            "--batch_size", "384",           # FD003ä¸FD001ç±»ä¼¼ï¼Œä½¿ç”¨ç›¸åŒé…ç½®
        #            "--epochs", "50",
        #            "--learning_rate", "0.005",      
        #            "--weight_decay", "0.00005",     
        #            # é—¨æ§CNN-TSMixeræœ€ä½³å‚æ•°
        #            "--patch", "5",                  
        #            "--cnn_channels", "64",          
        #            "--cnn_layers", "2",             
        #            "--cnn_kernel", "3",             
        #            "--d_model", "128",              
        #            "--depth", "4",                  
        #            "--token_mlp_dim", "256",        
        #            "--channel_mlp_dim", "128",      
        #            "--dropout", "0.05",             
        #            "--cnn_pool", "mean",            
        #            "--use_groupnorm",               
        #            "--gn_groups", "8",              
        #            "--scheduler", "plateau",        
        #            "--early_stopping", "10"         
        #            ]
        # },
        
        # {
        #     "name": "ğŸ¥‡ é—¨æ§CNN-TSMixeræœ€ä½³é…ç½® + FD004",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD004", 
        #            "--batch_size", "192",           # FD004æœ€å¤æ‚ï¼Œè¿›ä¸€æ­¥å‡å°æ‰¹é‡
        #            "--epochs", "80",                # FD004éœ€è¦æœ€å¤šè®­ç»ƒè½®æ•°
        #            "--learning_rate", "0.002",      # FD004ç”¨æ›´ä¿å®ˆå­¦ä¹ ç‡
        #            "--weight_decay", "0.0002",      # FD004å¢åŠ æ›´å¤šæƒé‡è¡°å‡
        #            # é—¨æ§CNN-TSMixeræœ€ä½³å‚æ•°
        #            "--patch", "5",                  
        #            "--cnn_channels", "64",          
        #            "--cnn_layers", "2",             
        #            "--cnn_kernel", "3",             
        #            "--d_model", "128",              
        #            "--depth", "4",                  
        #            "--token_mlp_dim", "256",        
        #            "--channel_mlp_dim", "128",      
        #            "--dropout", "0.10",             # FD004æœ€é«˜dropouté˜²è¿‡æ‹Ÿåˆ
        #            "--cnn_pool", "mean",            
        #            "--use_groupnorm",               
        #            "--gn_groups", "8",              
        #            "--scheduler", "plateau",        
        #            "--early_stopping", "15"         # FD004æœ€é«˜æ—©åœè€å¿ƒå€¼
        #            ]
        # },

        # # ============================================================================
        # # FD002ä¸“é¡¹ä¼˜åŒ–å®éªŒ - é’ˆå¯¹å¤šå·¥å†µå•æ•…éšœæ¨¡å¼çš„æ€§èƒ½æå‡
        # # ============================================================================
        
        # {
        #     "name": "ğŸ¯ FD002ä¼˜åŒ–-1: å¼ºæ­£åˆ™åŒ–é…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD002", 
        #            "--batch_size", "128",           # å‡å°æ‰¹é‡æå‡æ³›åŒ–
        #            "--epochs", "80",                # æ›´å¤šè½®æ•°
        #            "--learning_rate", "0.002",      # æ›´ä¿å®ˆå­¦ä¹ ç‡
        #            "--weight_decay", "0.0003",      # æ›´å¼ºæƒé‡è¡°å‡
        #            # å¼ºæ­£åˆ™åŒ–å‚æ•°
        #            "--patch", "5",                  
        #            "--cnn_channels", "64",          
        #            "--cnn_layers", "2",             
        #            "--cnn_kernel", "3",             
        #            "--d_model", "128",              
        #            "--depth", "4",                  
        #            "--token_mlp_dim", "256",        
        #            "--channel_mlp_dim", "128",      
        #            "--dropout", "0.15",             # å¤§å¹…å¢åŠ dropout
        #            "--cnn_pool", "mean",            
        #            "--use_groupnorm",               
        #            "--gn_groups", "8",              
        #            "--scheduler", "plateau",        
        #            "--early_stopping", "15"         # æ›´å¤§è€å¿ƒå€¼
        #            ]
        # },
        
        # {
        #     "name": "ğŸš€ FD002ä¼˜åŒ–-2: å¢å¼ºCNNç‰¹å¾æå–",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD002", 
        #            "--batch_size", "192",           # é€‚ä¸­æ‰¹é‡
        #            "--epochs", "70",                
        #            "--learning_rate", "0.0025",     # é€‚ä¸­å­¦ä¹ ç‡
        #            "--weight_decay", "0.0002",      
        #            # å¢å¼ºCNNé…ç½®
        #            "--patch", "4",                  # æ›´ç»†ç²’åº¦patch
        #            "--cnn_channels", "96",          # æ›´å¤šCNNé€šé“
        #            "--cnn_layers", "3",             # æ›´æ·±CNNå±‚
        #            "--cnn_kernel", "3",             
        #            "--d_model", "160",              # æ›´å¤§æ¨¡å‹å®¹é‡
        #            "--depth", "5",                  # æ›´æ·±TSMixer
        #            "--token_mlp_dim", "320",        
        #            "--channel_mlp_dim", "160",      
        #            "--dropout", "0.12",             # é€‚ä¸­dropout
        #            "--cnn_pool", "weighted",        # åŠ æƒæ± åŒ–å…³æ³¨åæœŸ
        #            "--use_groupnorm",               
        #            "--gn_groups", "12",             # æ›´å¤šGroupNormç»„
        #            "--scheduler", "onecycle",       # OneCycleè°ƒåº¦å™¨
        #            "--early_stopping", "12"         
        #            ]
        # },
        
        # {
        #     "name": "âš¡ FD002ä¼˜åŒ–-3: ç¨³å®šè®­ç»ƒé…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD002", 
        #            "--batch_size", "160",           # ä¸­å°æ‰¹é‡å¹³è¡¡
        #            "--epochs", "100",               # å……è¶³è®­ç»ƒè½®æ•°
        #            "--learning_rate", "0.0015",     # æ›´ç¨³å®šå­¦ä¹ ç‡
        #            "--weight_decay", "0.0001",      
        #            # ç¨³å®šè®­ç»ƒå‚æ•°
        #            "--patch", "6",                  # ç¨å¤§patchå‡å°‘tokenæ•°
        #            "--cnn_channels", "80",          # é€‚ä¸­CNNé€šé“
        #            "--cnn_layers", "2",             
        #            "--cnn_kernel", "3",             
        #            "--d_model", "144",              # ç¨å¤§æ¨¡å‹ç»´åº¦
        #            "--depth", "4",                  
        #            "--token_mlp_dim", "288",        
        #            "--channel_mlp_dim", "144",      
        #            "--dropout", "0.10",             
        #            "--cnn_pool", "mean",            
        #            "--use_groupnorm",               
        #            "--gn_groups", "10",             # é€‚ä¸­GroupNormç»„æ•°
        #            "--scheduler", "cosine",         # Cosineè°ƒåº¦å™¨é•¿æœŸç¨³å®š
        #            "--early_stopping", "20"         # æ›´å¤§è€å¿ƒå€¼é˜²æ—©åœ
        #            ]
        # },

        # ============================================================================
        # FD004é«˜çº§ä¼˜åŒ–é…ç½® - åŸºäºtokenæ•°é‡å’Œæ—¶é—´åˆ†è¾¨ç‡çš„ç²¾ç»†è°ƒä¼˜ï¼ˆå¤šå·¥å†µ+å¤šæ•…éšœï¼‰
        # ============================================================================
        
        # {
        #     "name": "ğŸ¯ FD004-Stable-Conditioned: ç¨³å®š10tokené…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD004", 
        #            "--batch_size", "192",           # FD004æ›´å¤æ‚ï¼Œå‡å°æ‰¹é‡
        #            "--epochs", "80",                # FD004éœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°
        #            "--learning_rate", "0.001",      # FD004ç”¨æ›´ä¿å®ˆå­¦ä¹ ç‡
        #            "--weight_decay", "0.0002",      # FD004å¢åŠ æƒé‡è¡°å‡
        #            # ç¨³å®š10tokené…ç½® (50/5=10 tokens, FD004çª—å£=50)
        #            "--patch", "5",                  # FD004çª—å£50ï¼Œpatch=5è·å¾—10ä¸ªtokens
        #            "--cnn_channels", "64",
        #            "--cnn_layers", "2",
        #            "--cnn_kernel", "3",
        #            "--d_model", "160",              # é€‚ä¸­æ¨¡å‹ç»´åº¦
        #            "--depth", "5",                  # é€‚ä¸­æ·±åº¦
        #            "--token_mlp_dim", "320",
        #            "--channel_mlp_dim", "160",
        #            "--dropout", "0.15",             # FD004éœ€è¦æ›´å¼ºæ­£åˆ™åŒ–
        #            "--cnn_pool", "weighted",        # åŠ æƒæ± åŒ–å…³æ³¨åæœŸ
        #            "--use_groupnorm",
        #            "--gn_groups", "8",
        #            "--scheduler", "cosine",         # é•¿æœŸç¨³å®šè°ƒåº¦
        #            "--early_stopping", "15"         # FD004éœ€è¦æ›´å¤§è€å¿ƒå€¼
        #            ]
        # },
        
    

        # ============================================================================
        # TSMixer + FD004 ä¸“é¡¹ä¼˜åŒ–é…ç½® - é’ˆå¯¹æœ€å¤æ‚æ•°æ®é›†çš„å‚æ•°è°ƒä¼˜
        # ============================================================================
        
        # {
        #     "name": "ğŸ¯ TSMixer + FD004: å¼ºæ­£åˆ™åŒ–ç¨³å®šé…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD004", 
        #            "--batch_size", "128",           # FD004å¤æ‚æ•°æ®ç”¨å°æ‰¹é‡
        #            "--epochs", "100",               # FD004éœ€è¦å……åˆ†è®­ç»ƒ
        #            "--learning_rate", "0.001",      # FD004ç”¨ä¿å®ˆå­¦ä¹ ç‡
        #            "--weight_decay", "0.0003",      # FD004éœ€è¦å¼ºæƒé‡è¡°å‡
        #            "--tsmixer_layers", "5",         # é€‚ä¸­å±‚æ•°å¹³è¡¡å®¹é‡å’Œè¿‡æ‹Ÿåˆ
        #            "--time_expansion", "6",         # è¾ƒå¤§æ—¶é—´æ‰©å±•å¤„ç†å¤æ‚æ—¶åº
        #            "--feat_expansion", "4",         # é€‚ä¸­ç‰¹å¾æ‰©å±•
        #            "--dropout", "0.20",             # FD004éœ€è¦å¼ºæ­£åˆ™åŒ–é˜²è¿‡æ‹Ÿåˆ
        #            "--scheduler", "cosine",         # é•¿æœŸç¨³å®šè®­ç»ƒ
        #            "--early_stopping", "15"         # FD004éœ€è¦æ›´å¤§è€å¿ƒå€¼
        #            ]
        # },
        
        # {
        #     "name": "ğŸš€ TSMixer + FD004: æ·±å±‚ç½‘ç»œé…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD004", 
        #            "--batch_size", "96",            # æ›´å°æ‰¹é‡é€‚åº”æ·±å±‚ç½‘ç»œ
        #            "--epochs", "120",               # æ·±å±‚ç½‘ç»œéœ€è¦æ›´å¤šè®­ç»ƒ
        #            "--learning_rate", "0.0008",     # æ·±å±‚ç½‘ç»œç”¨æ›´ä¿å®ˆå­¦ä¹ ç‡
        #            "--weight_decay", "0.0004",      # æ·±å±‚ç½‘ç»œå¢åŠ æƒé‡è¡°å‡
        #            "--tsmixer_layers", "8",         # æ›´æ·±çš„ç½‘ç»œ
        #            "--time_expansion", "8",         # æ›´å¤§æ—¶é—´æ‰©å±•
        #            "--feat_expansion", "5",         # æ›´å¤§ç‰¹å¾æ‰©å±•
        #            "--dropout", "0.25",             # æ·±å±‚ç½‘ç»œéœ€è¦æ›´å¼ºæ­£åˆ™åŒ–
        #            "--scheduler", "onecycle",       # æ·±å±‚ç½‘ç»œé€‚åˆOneCycle
        #            "--early_stopping", "20"         # æ·±å±‚ç½‘ç»œéœ€è¦æ›´å¤šè€å¿ƒ
        #            ]
        # },
        
        # {
        #     "name": "âš¡ TSMixer + FD004: é«˜æ•ˆè½»é‡é…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD004", 
        #            "--batch_size", "192",           # è½»é‡æ¨¡å‹å¯ç”¨è¾ƒå¤§æ‰¹é‡
        #            "--epochs", "80",                # è½»é‡æ¨¡å‹è®­ç»ƒæ›´å¿«
        #            "--learning_rate", "0.0015",     # è½»é‡æ¨¡å‹å¯ç”¨ç¨é«˜å­¦ä¹ ç‡
        #            "--weight_decay", "0.0002",      # é€‚ä¸­æƒé‡è¡°å‡
        #            "--tsmixer_layers", "3",         # è¾ƒæµ…ç½‘ç»œ
        #            "--time_expansion", "4",         # é€‚ä¸­æ—¶é—´æ‰©å±•
        #            "--feat_expansion", "3",         # é€‚ä¸­ç‰¹å¾æ‰©å±•
        #            "--dropout", "0.15",             # é€‚ä¸­æ­£åˆ™åŒ–
        #            "--scheduler", "plateau",        # å¿«é€Ÿå“åº”è°ƒåº¦å™¨
        #            "--early_stopping", "12"         # é€‚ä¸­æ—©åœè€å¿ƒ
        #            ]
        # },
        
        # {
        #     "name": "ğŸš€ FD004-Deeper-Token12: æ·±åº¦12tokené…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD004", 
        #            "--batch_size", "160",           # FD004æ·±å±‚æ¨¡å‹ç”¨æ›´å°æ‰¹é‡
        #            "--epochs", "90",                # FD004éœ€è¦æ›´å¤šè½®æ•°
        #            "--learning_rate", "0.0008",     # FD004æ·±å±‚æ¨¡å‹æ›´ä¿å®ˆå­¦ä¹ ç‡
        #            "--weight_decay", "0.0002",      # å¢åŠ æƒé‡è¡°å‡
        #            # æ·±åº¦12tokené…ç½® (50//4=12 tokens, T_eff=48)
        #            "--patch", "4",                  # æ›´ç»†ç²’åº¦patchè·å¾—12ä¸ªtokens
        #            "--cnn_channels", "80",          # æ›´å¤šCNNé€šé“
        #            "--cnn_layers", "3",             # æ›´æ·±CNNå±‚
        #            "--cnn_kernel", "3",
        #            "--d_model", "192",              # æ›´å¤§æ¨¡å‹å®¹é‡
        #            "--depth", "6",                  # æ›´æ·±TSMixer
        #            "--token_mlp_dim", "384",
        #            "--channel_mlp_dim", "192",
        #            "--dropout", "0.18",             # FD004æ·±å±‚æ¨¡å‹éœ€è¦æ›´å¼ºæ­£åˆ™åŒ–
        #            "--cnn_pool", "weighted",        # æœ«ç«¯æ•æ„Ÿæ± åŒ–
        #            "--use_groupnorm",
        #            "--gn_groups", "10",             # æ›´å¤šGroupNormç»„
        #            "--scheduler", "cosine",
        #            "--early_stopping", "18"         # FD004æ·±å±‚æ¨¡å‹éœ€è¦æ›´å¤šè€å¿ƒ
        #            ]
        # },
        
        # {
        #     "name": "âš¡ FD004-Light-Fast: è½»é‡16tokené«˜æ•ˆé…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer_gated", 
        #            "--fault", "FD004", 
        #            "--batch_size", "224",           # FD004è½»é‡ç‰ˆé€‚ä¸­æ‰¹é‡
        #            "--epochs", "70",                # FD004éœ€è¦æ›´å¤šè½®æ•°
        #            "--learning_rate", "0.0012",     # FD004ç¨ä¿å®ˆå­¦ä¹ ç‡
        #            "--weight_decay", "0.0002",      # FD004å¢åŠ æ­£åˆ™åŒ–
        #            # è½»é‡16tokené…ç½® (50//3=16 tokens, T_eff=48)
        #            "--patch", "3",                  # å°patchè·å¾—16ä¸ªtokens
        #            "--cnn_channels", "64",          # è½»é‡CNNé€šé“
        #            "--cnn_layers", "2",
        #            "--cnn_kernel", "3",
        #            "--d_model", "144",              # å¹³è¡¡çš„æ¨¡å‹ç»´åº¦
        #            "--depth", "5",                  # é€‚ä¸­æ·±åº¦
        #            "--token_mlp_dim", "288",
        #            "--channel_mlp_dim", "144",
        #            "--dropout", "0.20",             # FD004è½»é‡æ¨¡å‹éœ€è¦æ›´å¼ºdropout
        #            "--cnn_pool", "weighted",
        #            "--use_groupnorm",
        #            "--gn_groups", "8",
        #            "--scheduler", "plateau",        # å¿«é€Ÿå“åº”çš„plateauè°ƒåº¦
        #            "--early_stopping", "15"         # FD004éœ€è¦æ›´å¤šè€å¿ƒ
        #            ]
        # },
        
        # # TSMixerå®éªŒ
        # {
        #     "name": "TSMixer + FD001 (OneCycle + æ—©åœ)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "80",  # å¢åŠ æœ€å¤§epochsï¼Œè®©æ—©åœå†³å®šä½•æ—¶åœæ­¢
        #            "--learning_rate", "0.001",  # é™ä½å­¦ä¹ ç‡
        #            "--tsmixer_layers", "3",  # å‡å°‘å¤æ‚åº¦
        #            "--time_expansion", "3",
        #            "--feat_expansion", "3",
        #            "--dropout", "0.15",  # å¢åŠ æ­£åˆ™åŒ–
        #            "--scheduler", "onecycle",
        #            "--early_stopping", "10"  # 10è½®ä¸æ”¹å–„å°±åœæ­¢
        #            ]
        # },
        
        # {
        #     "name": "TSMixer + FD001 (Cosineè°ƒåº¦å™¨)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "128",
        #            "--epochs", "80",
        #            "--learning_rate", "0.002",  # é€‚ä¸­çš„åˆå§‹å­¦ä¹ ç‡
        #            "--tsmixer_layers", "6",
        #            "--time_expansion", "6",
        #            "--feat_expansion", "4",
        #            "--dropout", "0.15",
        #            "--scheduler", "cosine"  # ä½¿ç”¨Cosineè°ƒåº¦å™¨
        #            ]
        # },
        
        # {
        #     "name": "TSMixer + FD001 (Plateau + æ—©åœ3)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "100",  # æ›´å¤šepochsé…åˆæ—©åœ
        #            "--learning_rate", "0.002",
        #            "--tsmixer_layers", "4",
        #            "--time_expansion", "4",
        #            "--feat_expansion", "4",
        #            "--dropout", "0.1",
        #            "--scheduler", "plateau",
        #            "--early_stopping", "3"  # æ›´ä¸¥æ ¼çš„æ—©åœ
        #            ]
        # },
        
        # {
        #     "name": "TSMixer + FD001 (æ— æ—©åœå¯¹æ¯”)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "30",  # è¾ƒå°‘epochs
        #            "--learning_rate", "0.001",
        #            "--tsmixer_layers", "3",
        #            "--time_expansion", "3",
        #            "--feat_expansion", "3",
        #            "--dropout", "0.15",
        #            "--scheduler", "cosine",
        #            "--early_stopping", "0"  # ç¦ç”¨æ—©åœ
        #            ]
        # },
        
        # {
        #     "name": "TSMixer + FD002 (å¤æ‚æ•°æ®)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD002", 
        #            "--batch_size", "128",
        #            "--epochs", "100",
        #            "--learning_rate", "0.001",
        #            "--tsmixer_layers", "5",
        #            "--time_expansion", "8",
        #            "--feat_expansion", "6",
        #            "--dropout", "0.2",
        #            "--scheduler", "onecycle"
        #            ]
        # },
        
        # RBM-LSTMå®éªŒ - å¯»æ‰¾æœ€ä¼˜é…ç½®
        # {
        #     "name": "RBM-LSTM-1: åŸºç¡€ä¼˜åŒ–é…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "rbmlstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "128",
        #            "--epochs", "60",
        #            "--learning_rate", "0.001",
        #            "--rbm_hidden", "128",
        #            "--lstm_hidden1", "128",
        #            "--lstm_hidden2", "64",
        #            "--ff_hidden", "32",
        #            "--dropout_lstm", "0.3",
        #            "--rbm_pool", "last",
        #            "--enable_rbm_pretrain",
        #            "--rbm_epochs", "5",
        #            "--rbm_lr", "0.005",
        #            "--rbm_cd_k", "1",
        #            "--scheduler", "onecycle",
        #            "--early_stopping", "12"
        #            ]
        # },
        
        # {
        #     "name": "RBM-LSTM-2: æ›´å¤§æ¨¡å‹",
        #     "cmd": ["python", "train.py", 
        #            "--model", "rbmlstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "96",         # å‡å°batch_sizeé€‚åº”å¤§æ¨¡å‹
        #            "--epochs", "60",
        #            "--learning_rate", "0.0008",  # å¤§æ¨¡å‹ç”¨ç¨å°å­¦ä¹ ç‡
        #            "--rbm_hidden", "256",        # æ›´å¤§çš„RBM
        #            "--lstm_hidden1", "256",      # æ›´å¤§çš„LSTM
        #            "--lstm_hidden2", "128",
        #            "--ff_hidden", "64",          # æ›´å¤§çš„å‰é¦ˆå±‚
        #            "--dropout_lstm", "0.4",      # å¤§æ¨¡å‹éœ€è¦æ›´å¤šæ­£åˆ™åŒ–
        #            "--rbm_pool", "last",
        #            "--enable_rbm_pretrain",
        #            "--rbm_epochs", "6",          # æ›´å¤šé¢„è®­ç»ƒ
        #            "--rbm_lr", "0.003",          # æ›´å°çš„RBMå­¦ä¹ ç‡
        #            "--rbm_cd_k", "1",
        #            "--scheduler", "onecycle",
        #            "--early_stopping", "15"
        #            ]
        # },
        
        # {
        #     "name": "RBM-LSTM-3: æ·±å±‚ç½‘ç»œ",
        #     "cmd": ["python", "train.py", 
        #            "--model", "rbmlstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "128",
        #            "--epochs", "60",
        #            "--learning_rate", "0.0012",  # ç¨é«˜å­¦ä¹ ç‡
        #            "--rbm_hidden", "128",
        #            "--lstm_hidden1", "128",
        #            "--lstm_hidden2", "128",      # ä¸¤å±‚LSTMç›¸åŒå¤§å°
        #            "--ff_hidden", "64",          # æ›´å¤§å‰é¦ˆå±‚
        #            "--dropout_lstm", "0.25",     # ç¨ä½dropout
        #            "--rbm_pool", "mean",         # å°è¯•å‡å€¼æ± åŒ–
        #            "--enable_rbm_pretrain",
        #            "--rbm_epochs", "4",
        #            "--rbm_lr", "0.008",          # ç¨é«˜RBMå­¦ä¹ ç‡
        #            "--rbm_cd_k", "1",
        #            "--scheduler", "onecycle",
        #            "--early_stopping", "12"
        #            ]
        # },
        
        # {
        #     "name": "RBM-LSTM-4: é«˜å­¦ä¹ ç‡é…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "rbmlstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "160",        # æ›´å¤§batch
        #            "--epochs", "50",
        #            "--learning_rate", "0.0015",  # æ›´é«˜å­¦ä¹ ç‡
        #            "--rbm_hidden", "128",
        #            "--lstm_hidden1", "128",
        #            "--lstm_hidden2", "64",
        #            "--ff_hidden", "32",
        #            "--dropout_lstm", "0.2",      # æ›´ä½dropout
        #            "--rbm_pool", "last",
        #            "--enable_rbm_pretrain",
        #            "--rbm_epochs", "3",          # è¾ƒå°‘é¢„è®­ç»ƒè½®æ•°
        #            "--rbm_lr", "0.01",           # è¾ƒé«˜RBMå­¦ä¹ ç‡
        #            "--rbm_cd_k", "1",
        #            "--scheduler", "onecycle",
        #            "--early_stopping", "10"
        #            ]
        # },
        
        # {
        #     "name": "RBM-LSTM-5: æ— é¢„è®­ç»ƒåŸºçº¿",
        #     "cmd": ["python", "train.py", 
        #            "--model", "rbmlstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "128",
        #            "--epochs", "60",
        #            "--learning_rate", "0.001",
        #            "--rbm_hidden", "128",
        #            "--lstm_hidden1", "128",
        #            "--lstm_hidden2", "64",
        #            "--ff_hidden", "32",
        #            "--dropout_lstm", "0.3",
        #            "--rbm_pool", "last",
        #            # ä¸å¯ç”¨RBMé¢„è®­ç»ƒ
        #            "--scheduler", "onecycle",
        #            "--early_stopping", "12"
        #            ]
        # },
        
        # {
        #     "name": "RBM-LSTM-6: Cosineè°ƒåº¦å™¨",
        #     "cmd": ["python", "train.py", 
        #            "--model", "rbmlstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "128",
        #            "--epochs", "80",             # æ›´å¤šè½®æ•°é…åˆcosine
        #            "--learning_rate", "0.002",   # æ›´é«˜åˆå§‹å­¦ä¹ ç‡
        #            "--rbm_hidden", "128",
        #            "--lstm_hidden1", "128",
        #            "--lstm_hidden2", "64",
        #            "--ff_hidden", "32",
        #            "--dropout_lstm", "0.35",
        #            "--rbm_pool", "last",
        #            "--enable_rbm_pretrain",
        #            "--rbm_epochs", "5",
        #            "--rbm_lr", "0.005",
        #            "--rbm_cd_k", "1",
        #            "--scheduler", "cosine",      # ä½¿ç”¨cosineè°ƒåº¦å™¨
        #            "--early_stopping", "15"
        #            ]
        #         },
        
        # # TSMixer ä¼˜åŒ–å®éªŒ - åŸºäºæ—¥å¿—åˆ†æ
        # {
        #     "name": "TSMixer-1: æœ€ä¼˜åŸºçº¿é…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "60",
        #            "--learning_rate", "0.002",      # éªŒè¯æœ‰æ•ˆçš„é«˜å­¦ä¹ ç‡
        #            "--tsmixer_layers", "4",         # æœ€ä½³å±‚æ•°
        #            "--time_expansion", "4",
        #            "--feat_expansion", "4",
        #            "--dropout", "0.1",              # è¾ƒä½dropout
        #            "--scheduler", "plateau",        # æœ€ä½³è°ƒåº¦å™¨
        #            "--early_stopping", "5"          # ç¨å¾®å¢åŠ è€å¿ƒå€¼
        #            ]
        # },
        
        # {
        #     "name": "TSMixer-2: å¼ºæ­£åˆ™åŒ–é…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "128",           # å‡å°batch size
        #            "--epochs", "60",
        #            "--learning_rate", "0.0015",     # ç¨ä½å­¦ä¹ ç‡
        #            "--weight_decay", "0.0005",      # å¢å¼ºæƒé‡è¡°å‡
        #            "--tsmixer_layers", "4",
        #            "--time_expansion", "3",         # å‡å°‘å¤æ‚åº¦
        #            "--feat_expansion", "3",
        #            "--dropout", "0.2",              # æ›´å¼ºdropout
        #            "--scheduler", "plateau",
        #            "--early_stopping", "8"
        #            ]
        # },
        
        # {
        #     "name": "TSMixer-3: æ·±å±‚ç½‘ç»œé…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "192",
        #            "--epochs", "80",
        #            "--learning_rate", "0.001",      # æ·±å±‚ç½‘ç»œç”¨è¾ƒä½å­¦ä¹ ç‡
        #            "--weight_decay", "0.0002",
        #            "--tsmixer_layers", "6",         # æ›´æ·±ç½‘ç»œ
        #            "--time_expansion", "5",
        #            "--feat_expansion", "4",
        #            "--dropout", "0.15",
        #            "--scheduler", "onecycle",       # æ·±å±‚ç½‘ç»œé€‚åˆonecycle
        #            "--early_stopping", "10"
        #            ]
        # },
        
        # {
        #     "name": "TSMixer-4: ä¿å®ˆè®­ç»ƒé…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "100",
        #            "--learning_rate", "0.0008",     # ä¿å®ˆå­¦ä¹ ç‡
        #            "--weight_decay", "0.0003",
        #            "--tsmixer_layers", "3",         # è¾ƒæµ…ç½‘ç»œ
        #            "--time_expansion", "4",
        #            "--feat_expansion", "4",
        #            "--dropout", "0.25",             # å¼ºæ­£åˆ™åŒ–
        #            "--scheduler", "cosine",         # é•¿æœŸè®­ç»ƒç”¨cosine
        #            "--early_stopping", "15"
        #            ]
        # },
        
        # {
        #     "name": "TSMixer-5: é«˜æ•ˆå¿«é€Ÿé…ç½®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "384",           # å¤§batchå¿«é€Ÿè®­ç»ƒ
        #            "--epochs", "40",                # è¾ƒå°‘è½®æ•°
        #            "--learning_rate", "0.003",      # æ›´é«˜å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›
        #            "--weight_decay", "0.0001",
        #            "--tsmixer_layers", "4",
        #            "--time_expansion", "4",
        #            "--feat_expansion", "4",
        #            "--dropout", "0.1",
        #            "--scheduler", "plateau",
        #            "--early_stopping", "3"          # æ¿€è¿›æ—©åœ
        #            ]
        # },
        
        # {
        #     "name": "TSMixer-6: FD002å¤æ‚æ•°æ®",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD002", 
        #            "--batch_size", "128",
        #            "--epochs", "100",
        #            "--learning_rate", "0.001",      # FD002ç”¨è¾ƒä¿å®ˆå‚æ•°
        #            "--weight_decay", "0.0005",
        #            "--tsmixer_layers", "5",         # å¤æ‚æ•°æ®éœ€è¦æ›´å¤šå±‚
        #            "--time_expansion", "6",
        #            "--feat_expansion", "5",
        #            "--dropout", "0.2",              # æ›´å¼ºæ­£åˆ™åŒ–
        #            "--scheduler", "onecycle",
        #            "--early_stopping", "12"
        #            ]
        # },
        
        # ============================================================================
        # TSMixer ä¼˜åŒ–å®éªŒ - åŸºäºæ—¥å¿—åˆ†æçš„æ–°é…ç½®
        # ============================================================================
        
        # {
        #     "name": "TSMixer + FD001 (æ·±å±‚ä¼˜åŒ–é…ç½®)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "100",
        #            "--learning_rate", "0.0012",
        #            "--weight_decay", "0.0002",
        #            "--tsmixer_layers", "8",         # æ›´æ·±çš„ç½‘ç»œ
        #            "--time_expansion", "6",         # æ›´å¤§çš„æ—¶é—´æ‰©å±•
        #            "--feat_expansion", "4",
        #            "--dropout", "0.12",             # ç²¾è°ƒçš„dropout
        #            "--scheduler", "onecycle",       # æœ€ä½³è°ƒåº¦å™¨
        #            "--early_stopping", "12"
        #            ]
        # },
        
        # {
        #     "name": "TSMixer + FD001 (å¿«é€Ÿé«˜æ•ˆé…ç½®)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "512",           # å¤§æ‰¹é‡
        #            "--epochs", "60",
        #            "--learning_rate", "0.004",      # é«˜å­¦ä¹ ç‡
        #            "--weight_decay", "0.0001",
        #            "--tsmixer_layers", "5",
        #            "--time_expansion", "4",
        #            "--feat_expansion", "5",         # å¹³è¡¡çš„æ‰©å±•
        #            "--dropout", "0.08",             # ä½dropout
        #            "--scheduler", "plateau",
        #            "--early_stopping", "5"
        #            ]
        # },
        
        # {
        #     "name": "TSMixer + FD001 (ææ·±ç½‘ç»œæµ‹è¯•)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "192",
        #            "--epochs", "120",
        #            "--learning_rate", "0.001",
        #            "--weight_decay", "0.0003",
        #            "--tsmixer_layers", "10",        # ææ·±ç½‘ç»œ
        #            "--time_expansion", "5",
        #            "--feat_expansion", "4",
        #            "--dropout", "0.15",             # é€‚ä¸­æ­£åˆ™åŒ–
        #            "--scheduler", "onecycle",
        #            "--early_stopping", "15"
        #            ]
        # },
        
        # ============================================================================
        # å‰ä¸‰åæœ€ä¼˜é…ç½® - åŸºäº13ä¸ªå®éªŒçš„æ€§èƒ½åˆ†æ
        # ============================================================================
        
        # {
        #     "name": "ğŸ¥‡ TSMixerå† å†›é…ç½® (11.46 RMSE)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "384",           # å¤§æ‰¹é‡è®­ç»ƒ
        #            "--epochs", "50",
        #            "--learning_rate", "0.005",      # æé«˜å­¦ä¹ ç‡
        #            "--weight_decay", "0.00005",     # æä½æƒé‡è¡°å‡
        #            "--tsmixer_layers", "4",         # è½»é‡ç½‘ç»œ
        #            "--time_expansion", "3",         # é€‚ä¸­æ—¶é—´æ‰©å±•
        #            "--feat_expansion", "4",         # é€‚ä¸­ç‰¹å¾æ‰©å±•
        #            "--dropout", "0.05",             # æä½dropout
        #            "--scheduler", "plateau",        # å¹³å°è°ƒåº¦å™¨
        #            "--early_stopping", "10"          # æ¿€è¿›æ—©åœ
        #            ]
        # },
        
        # {
        #     "name": "ğŸ¥ˆ TSMixeräºšå†›é…ç½® (11.69 RMSE)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "512",           # è¶…å¤§æ‰¹é‡
        #            "--epochs", "60",
        #            "--learning_rate", "0.004",      # é«˜å­¦ä¹ ç‡
        #            "--weight_decay", "0.0001",      # ä½æƒé‡è¡°å‡
        #            "--tsmixer_layers", "5",         # ä¸­ç­‰æ·±åº¦
        #            "--time_expansion", "4",         # å¹³è¡¡æ—¶é—´æ‰©å±•
        #            "--feat_expansion", "5",         # è¾ƒå¤§ç‰¹å¾æ‰©å±•
        #            "--dropout", "0.08",             # ä½dropout
        #            "--scheduler", "plateau",        # å¹³å°è°ƒåº¦å™¨
        #            "--early_stopping", "10"          # å¿«é€Ÿæ—©åœ
        #            ]
        # },
        
        # {
        #     "name": "ğŸ¥‰ TSMixerå­£å†›é…ç½® (11.91 RMSE)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",           # ä¸­ç­‰æ‰¹é‡
        #            "--epochs", "100",               # æ›´å¤šè®­ç»ƒè½®æ•°
        #            "--learning_rate", "0.0012",     # ç²¾è°ƒå­¦ä¹ ç‡
        #            "--weight_decay", "0.0002",      # é€‚ä¸­æƒé‡è¡°å‡
        #            "--tsmixer_layers", "8",         # æ·±å±‚ç½‘ç»œ
        #            "--time_expansion", "6",         # å¤§æ—¶é—´æ‰©å±•
        #            "--feat_expansion", "4",         # é€‚ä¸­ç‰¹å¾æ‰©å±•
        #            "--dropout", "0.12",             # é€‚ä¸­dropout
        #            "--scheduler", "onecycle",       # OneCycleè°ƒåº¦å™¨
        #            "--early_stopping", "12"         # è€å¿ƒæ—©åœ
        #            ]
        # },
        
        # ============================================================================
        # CNN-TSMixer å®éªŒé…ç½® - æ–°çš„æ··åˆæ¶æ„
        # ============================================================================
        
        # {
        #     "name": "ğŸš€ CNN-TSMixer + FD001 (åŸºç¡€é…ç½®)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "cnn_tsmixer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "128",
        #            "--epochs", "50",
        #            "--learning_rate", "0.001",
        #            "--weight_decay", "0.0001",
        #            # CNN-TSMixerç‰¹å®šå‚æ•°
        #            "--patch", "5",
        #            "--cnn_channels", "64",
        #            "--cnn_layers", "2",
        #            "--cnn_kernel", "5",
        #            "--d_model", "128",
        #            "--depth", "4",
        #            "--token_mlp_dim", "256",
        #            "--channel_mlp_dim", "128",
        #            "--dropout", "0.10",
        #            "--cnn_pool", "mean",
        #            "--scheduler", "cosine",
        #            "--early_stopping", "10"
        #            ]
        # },
        
        
        # {
        #     "name": "RBM-LSTM + FD001 (æ— é¢„è®­ç»ƒåŸºçº¿)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "rbmlstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "60",
        #            "--learning_rate", "0.0003",  # è®ºæ–‡å»ºè®®çš„AdamWå­¦ä¹ ç‡
        #            "--rbm_hidden", "64",
        #            "--lstm_hidden1", "64",
        #            "--lstm_hidden2", "64",
        #            "--ff_hidden", "8",
        #            "--dropout_lstm", "0.5",
        #            "--rbm_pool", "last",
        #            "--scheduler", "plateau",
        #            "--early_stopping", "7"
        #            ]
        # },
        
        # {
        #     "name": "RBM-LSTM + FD001 (RBMé¢„è®­ç»ƒ)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "rbmlstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "30",            # å‡å°‘epochsä¾¿äºæµ‹è¯•
        #            "--learning_rate", "0.0003",
        #            "--rbm_hidden", "64",
        #            "--lstm_hidden1", "64",
        #            "--lstm_hidden2", "64",
        #            "--ff_hidden", "8",
        #            "--dropout_lstm", "0.5",
        #            "--rbm_pool", "last",
        #            "--enable_rbm_pretrain",  # å¯ç”¨RBMé¢„è®­ç»ƒ
        #            "--rbm_epochs", "3",      # RBMé¢„è®­ç»ƒè½®æ•°
        #            "--rbm_lr", "0.01",       # RBMé¢„è®­ç»ƒå­¦ä¹ ç‡
        #            "--rbm_cd_k", "1",        # å¯¹æ¯”æ•£åº¦æ­¥æ•°
        #            "--scheduler", "plateau",
        #            "--early_stopping", "7"
        #            ]
        # },
        
        # {
        #     "name": "RBM-LSTM + FD001 (å¤§æ¨¡å‹+é¢„è®­ç»ƒ)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "rbmlstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "128",
        #            "--epochs", "80",
        #            "--learning_rate", "0.0002",  # å¤§æ¨¡å‹ç”¨è¾ƒå°å­¦ä¹ ç‡
        #            "--rbm_hidden", "128",         # æ›´å¤§çš„RBMéšè—å±‚
        #            "--lstm_hidden1", "128",       # æ›´å¤§çš„LSTMå±‚
        #            "--lstm_hidden2", "64",
        #            "--ff_hidden", "16",           # æ›´å¤§çš„å‰é¦ˆå±‚
        #            "--dropout_lstm", "0.6",       # æ›´å¼ºçš„æ­£åˆ™åŒ–
        #            "--rbm_pool", "mean",          # å°è¯•å¹³å‡æ± åŒ–
        #            "--enable_rbm_pretrain",
        #            "--rbm_epochs", "5",           # æ›´å¤šé¢„è®­ç»ƒè½®æ•°
        #            "--rbm_lr", "0.008",
        #            "--rbm_cd_k", "1",
        #            "--scheduler", "onecycle",
        #            "--early_stopping", "10"
        #            ]
        # },
        
        # {
        #     "name": "RBM-LSTM + FD002 (å¤æ‚æ•°æ®+é¢„è®­ç»ƒ)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "rbmlstm", 
        #            "--fault", "FD002", 
        #            "--batch_size", "128",
        #            "--epochs", "100",
        #            "--learning_rate", "0.0002",
        #            "--rbm_hidden", "96",
        #            "--lstm_hidden1", "96",
        #            "--lstm_hidden2", "64",
        #            "--ff_hidden", "12",
        #            "--dropout_lstm", "0.6",       # FD002æ›´å¤æ‚ï¼Œéœ€è¦æ›´å¼ºæ­£åˆ™åŒ–
        #            "--rbm_pool", "last",
        #            "--enable_rbm_pretrain",
        #            "--rbm_epochs", "4",
        #            "--rbm_lr", "0.01",
        #            "--rbm_cd_k", "1",
        #            "--scheduler", "cosine",
        #            "--early_stopping", "8"
        #            ]
        # },

        # BiLSTMå®éªŒï¼ˆæ³¨é‡Šæ‰ä»¥ä¾¿ä¸“æ³¨æµ‹è¯•RBM-LSTMï¼‰
        # {
        #     "name": "BiLSTM + FD001 (åŸºç¡€é…ç½®)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "bilstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "60",
        #            "--learning_rate", "0.001",
        #            "--lstm_hidden", "64",
        #            "--lstm_layers", "2",
        #            "--bidirectional",  # åŒå‘LSTM
        #            "--mlp_hidden", "64",
        #            "--lstm_pool", "last",
        #            "--dropout", "0.1",
        #            "--scheduler", "plateau",
        #            "--early_stopping", "7"
        #            ]
        # },
        
        # {
        #     "name": "BiLSTM + FD001 (æ·±å±‚ç½‘ç»œ)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "bilstm", 
        #            "--fault", "FD001", 
        #            "--batch_size", "128",
        #            "--epochs", "80",
        #            "--learning_rate", "0.0008",
        #            "--lstm_hidden", "128",  # æ›´å¤§çš„éšè—å±‚
        #            "--lstm_layers", "3",    # æ›´å¤šå±‚æ•°
        #            "--bidirectional",
        #            "--mlp_hidden", "128",
        #            "--lstm_pool", "mean",   # å¹³å‡æ± åŒ–
        #            "--dropout", "0.15",
        #            "--scheduler", "onecycle",
        #            "--early_stopping", "10"
        #            ]
        # },
        
        # {
        #     "name": "BiLSTM + FD002 (å¤æ‚æ•°æ®)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "bilstm", 
        #            "--fault", "FD002", 
        #            "--batch_size", "128",
        #            "--epochs", "100",
        #            "--learning_rate", "0.0005",
        #            "--lstm_hidden", "96",
        #            "--lstm_layers", "2",
        #            "--bidirectional",
        #            "--mlp_hidden", "96",
        #            "--lstm_pool", "last",
        #            "--dropout", "0.2",
        #            "--scheduler", "cosine",
        #            "--early_stopping", "8"
        #            ]
        # },

        
        # {
        #     "name": "Transformer + FD001 (å°æ¨¡å‹)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "transformer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "100",
        #            "--learning_rate", "0.0003",
        #            "--d_model", "128",
        #            "--nhead", "8",
        #            "--num_layers", "6",
        #            ]
        # },
        
        # {
        #     "name": "Transformer + FD001 (ä¸­ç­‰æ¨¡å‹)",
        #     "cmd": ["python", "train.py", 
        #            "--model", "transformer", 
        #            "--fault", "FD001", 
        #            "--batch_size", "256",
        #            "--epochs", "30",
        #            "--learning_rate", "0.0003",
        #            "--d_model", "128",
        #            "--nhead", "4",
        #            "--num_layers", "3",
        #            ]
        # },
        
       
        # {
        #     "name": "Transformer + FD002",
        #     "cmd": ["python", "train.py", 
        #            "--model", "transformer", 
        #            "--fault", "FD002", 
        #            "--batch_size", "256",
        #            "--epochs", "40",  # FD002æ›´å¤æ‚ï¼Œéœ€è¦æ›´å¤šè½®æ¬¡
        #            "--learning_rate", "0.0002",  # ç¨å¾®é™ä½å­¦ä¹ ç‡
        #            "--d_model", "128",
        #            "--nhead", "4",
        #            "--num_layers", "3",
        #            ]
        # }
    ]
    
    print("å¼€å§‹æ‰¹é‡å®éªŒ...")
    results = {}
    
    for i, exp in enumerate(experiments, 1):
        print(f"\n\nğŸš€ å®éªŒ {i}/{len(experiments)}: {exp['name']}")
        
        success = run_command(exp["cmd"])
        results[exp["name"]] = "æˆåŠŸ" if success else "å¤±è´¥"
        
        if not success:
            print(f"âŒ å®éªŒå¤±è´¥: {exp['name']}")
            choice = input("ç»§ç»­ä¸‹ä¸€ä¸ªå®éªŒï¼Ÿ(y/n): ").lower()
            if choice != 'y':
                break
        else:
            print(f"âœ… å®éªŒæˆåŠŸ: {exp['name']}")
    
    # æ‰“å°æ€»ç»“
    print(f"\n\n{'='*60}")
    print("å®éªŒæ€»ç»“:")
    print('='*60)
    for name, status in results.items():
        status_icon = "âœ…" if status == "æˆåŠŸ" else "âŒ"
        print(f"{status_icon} {name}: {status}")


if __name__ == "__main__":
    main()
